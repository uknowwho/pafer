{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853d76de-66f0-4685-ac8f-e13c4eddb864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f279bcd4-fe8c-43fa-b633-4e949bc6ab57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>marcu brown</td>\n",
       "      <td>marcu</td>\n",
       "      <td>brown</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-01-21</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>bouthy pierrelouis</td>\n",
       "      <td>bouthy</td>\n",
       "      <td>pierrelouis</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973-01-22</td>\n",
       "      <td>43</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>10996</td>\n",
       "      <td>steven butler</td>\n",
       "      <td>steven</td>\n",
       "      <td>butler</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>Male</td>\n",
       "      <td>1992-07-17</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>2013-11-22</td>\n",
       "      <td>2013-11-24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>10997</td>\n",
       "      <td>malcolm simmons</td>\n",
       "      <td>malcolm</td>\n",
       "      <td>simmons</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-03-25</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>2014-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>10999</td>\n",
       "      <td>winston gregory</td>\n",
       "      <td>winston</td>\n",
       "      <td>gregory</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1958-10-01</td>\n",
       "      <td>57</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>808</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>11000</td>\n",
       "      <td>farrah jean</td>\n",
       "      <td>farrah</td>\n",
       "      <td>jean</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>Female</td>\n",
       "      <td>1982-11-17</td>\n",
       "      <td>33</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>2014-03-09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>11001</td>\n",
       "      <td>florencia sanmartin</td>\n",
       "      <td>florencia</td>\n",
       "      <td>sanmartin</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>Female</td>\n",
       "      <td>1992-12-18</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7214 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                 name      first         last  \\\n",
       "0         1     miguel hernandez     miguel    hernandez   \n",
       "1         3          kevon dixon      kevon        dixon   \n",
       "2         4             ed philo         ed        philo   \n",
       "3         5          marcu brown      marcu        brown   \n",
       "4         6   bouthy pierrelouis     bouthy  pierrelouis   \n",
       "...     ...                  ...        ...          ...   \n",
       "7209  10996        steven butler     steven       butler   \n",
       "7210  10997      malcolm simmons    malcolm      simmons   \n",
       "7211  10999      winston gregory    winston      gregory   \n",
       "7212  11000          farrah jean     farrah         jean   \n",
       "7213  11001  florencia sanmartin  florencia    sanmartin   \n",
       "\n",
       "     compas_screening_date     sex         dob  age          age_cat  \\\n",
       "0               2013-08-14    Male  1947-04-18   69  Greater than 45   \n",
       "1               2013-01-27    Male  1982-01-22   34          25 - 45   \n",
       "2               2013-04-14    Male  1991-05-14   24     Less than 25   \n",
       "3               2013-01-13    Male  1993-01-21   23     Less than 25   \n",
       "4               2013-03-26    Male  1973-01-22   43          25 - 45   \n",
       "...                    ...     ...         ...  ...              ...   \n",
       "7209            2013-11-23    Male  1992-07-17   23     Less than 25   \n",
       "7210            2014-02-01    Male  1993-03-25   23     Less than 25   \n",
       "7211            2014-01-14    Male  1958-10-01   57  Greater than 45   \n",
       "7212            2014-03-09  Female  1982-11-17   33          25 - 45   \n",
       "7213            2014-06-30  Female  1992-12-18   23     Less than 25   \n",
       "\n",
       "                  race  ...  v_decile_score  v_score_text  v_screening_date  \\\n",
       "0                Other  ...               1           Low        2013-08-14   \n",
       "1     African-American  ...               1           Low        2013-01-27   \n",
       "2     African-American  ...               3           Low        2013-04-14   \n",
       "3     African-American  ...               6        Medium        2013-01-13   \n",
       "4                Other  ...               1           Low        2013-03-26   \n",
       "...                ...  ...             ...           ...               ...   \n",
       "7209  African-American  ...               5        Medium        2013-11-23   \n",
       "7210  African-American  ...               5        Medium        2014-02-01   \n",
       "7211             Other  ...               1           Low        2014-01-14   \n",
       "7212  African-American  ...               2           Low        2014-03-09   \n",
       "7213          Hispanic  ...               4           Low        2014-06-30   \n",
       "\n",
       "      in_custody  out_custody  priors_count.1 start   end event two_year_recid  \n",
       "0     2014-07-07   2014-07-14               0     0   327     0              0  \n",
       "1     2013-01-26   2013-02-05               0     9   159     1              1  \n",
       "2     2013-06-16   2013-06-16               4     0    63     0              1  \n",
       "3            NaN          NaN               1     0  1174     0              0  \n",
       "4            NaN          NaN               2     0  1102     0              0  \n",
       "...          ...          ...             ...   ...   ...   ...            ...  \n",
       "7209  2013-11-22   2013-11-24               0     1   860     0              0  \n",
       "7210  2014-01-31   2014-02-02               0     1   790     0              0  \n",
       "7211  2014-01-13   2014-01-14               0     0   808     0              0  \n",
       "7212  2014-03-08   2014-03-09               3     0   754     0              0  \n",
       "7213  2015-03-15   2015-03-15               2     0   258     0              1  \n",
       "\n",
       "[7214 rows x 53 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the csv, looks fairly clean, source: https://github.com/propublica/compas-analysis/blob/master/\n",
    "compas = pd.read_csv(\"compas-scores-two-years.csv\")\n",
    "compas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4cd333-89c5-4aec-8ee1-5ad305897309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Male\n",
       "1         Male\n",
       "2         Male\n",
       "5         Male\n",
       "6         Male\n",
       "         ...  \n",
       "7209      Male\n",
       "7210      Male\n",
       "7211      Male\n",
       "7212    Female\n",
       "7213    Female\n",
       "Name: sex, Length: 6172, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy the pre-processing steps from \"Loading Data\" found on: https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb\n",
    "compas = compas[(compas[\"days_b_screening_arrest\"] <= 30) & (compas[\"days_b_screening_arrest\"] >= -30)]\n",
    "\n",
    "# nr of rows match those in link\n",
    "compas[\"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8bb0e8a-5f2a-4bff-b115-576fef103302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate labels\n",
    "compas_y = compas[\"two_year_recid\"]\n",
    "compas_X = compas.drop(\"two_year_recid\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158ce077-b10b-422a-816b-92bbcfb3a092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensure favorable prediction is 1 and vice versa\n",
    "compas_y = compas_y.map({0:1, 1:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51e42b2-99a5-41b0-a695-051a89a387cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop rows that contain no valuable information (id, name etc)\n",
    "compas_X = compas_X[[\"age\", \"c_charge_degree\", \"age_cat\", \"score_text\", \"sex\", \"priors_count\", \n",
    "                    \"days_b_screening_arrest\", \"decile_score\", \"race\", \"in_custody\", \"out_custody\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b5bf93-d27f-4a94-b319-d75dea19ad75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert dates to just years and numerical types\n",
    "def date_to_justyear(date):\n",
    "    if type(date) == str:\n",
    "        return int(date[:4])\n",
    "    \n",
    "    return date\n",
    "\n",
    "for column in [\"in_custody\", \"out_custody\"]:\n",
    "    compas_X[column] = compas_X[column].apply(func=date_to_justyear, convert_dtype=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e4cd06-49d5-4c96-8522-493fea907b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate sensitive attributes\n",
    "compas_sex = compas_X[\"sex\"]\n",
    "compas_race = compas_X[\"race\"]\n",
    "compas_age = compas_X[\"age\"]\n",
    "compas_age_cat = compas_X[\"age_cat\"]\n",
    "compas_X = compas_X.drop([\"race\", \"sex\", \"age\", \"age_cat\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79894f87-4a9d-4b9b-a8a8-5585b631c21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# distinguish between privileged and un-privileged ethnic groups\n",
    "compas_race = compas_race.map({\"Caucasian\": \"White\", \"African-American\": \"Non_White\", \"Hispanic\": \"Non_White\", \"Other\": \"Non_White\", \"Asian\": \"Non_White\", \"Native American\": \"Non_White\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a47cb8-d233-4fe9-83f1-849879c5751d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Male-Non_White\n",
       "1         Male-Non_White\n",
       "2         Male-Non_White\n",
       "5         Male-Non_White\n",
       "6             Male-White\n",
       "              ...       \n",
       "7209      Male-Non_White\n",
       "7210      Male-Non_White\n",
       "7211      Male-Non_White\n",
       "7212    Female-Non_White\n",
       "7213    Female-Non_White\n",
       "Length: 6172, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make intersectional sensitive feature\n",
    "compas_sexrace = pd.concat([compas_sex, compas_race], axis=1)\n",
    "compas_sex_race = compas_sexrace[['sex', 'race']].agg('-'.join, axis=1)\n",
    "compas_sex_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4285ca-d954-4310-a038-00008c19bf74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# impute the numerical missing values with the median\n",
    "compas_X = compas_X.fillna(compas_X.median(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b396e8-36ce-439b-bb3b-8ed009db6fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bin the numerical features into 5 equal-width bins\n",
    "bins = 5\n",
    "compas_X[\"priors_count\"] = pd.cut(compas_X[\"priors_count\"], bins=bins)\n",
    "compas_X[\"days_b_screening_arrest\"] = pd.cut(compas_X[\"days_b_screening_arrest\"], bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6609b761-9ce0-4baa-b843-018178d86e41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_charge_degree_F</th>\n",
       "      <th>c_charge_degree_M</th>\n",
       "      <th>score_text_High</th>\n",
       "      <th>score_text_Low</th>\n",
       "      <th>score_text_Medium</th>\n",
       "      <th>priors_count_(-0.038, 7.6]</th>\n",
       "      <th>priors_count_(7.6, 15.2]</th>\n",
       "      <th>priors_count_(15.2, 22.8]</th>\n",
       "      <th>priors_count_(22.8, 30.4]</th>\n",
       "      <th>priors_count_(30.4, 38.0]</th>\n",
       "      <th>...</th>\n",
       "      <th>in_custody_2009</th>\n",
       "      <th>in_custody_2013</th>\n",
       "      <th>in_custody_2014</th>\n",
       "      <th>in_custody_2015</th>\n",
       "      <th>in_custody_2016</th>\n",
       "      <th>out_custody_2013</th>\n",
       "      <th>out_custody_2014</th>\n",
       "      <th>out_custody_2015</th>\n",
       "      <th>out_custody_2016</th>\n",
       "      <th>out_custody_2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6172 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      c_charge_degree_F  c_charge_degree_M  score_text_High  score_text_Low  \\\n",
       "0                     1                  0                0               1   \n",
       "1                     1                  0                0               1   \n",
       "2                     1                  0                0               1   \n",
       "5                     0                  1                0               1   \n",
       "6                     1                  0                0               0   \n",
       "...                 ...                ...              ...             ...   \n",
       "7209                  1                  0                0               0   \n",
       "7210                  1                  0                0               1   \n",
       "7211                  1                  0                0               1   \n",
       "7212                  0                  1                0               1   \n",
       "7213                  1                  0                0               1   \n",
       "\n",
       "      score_text_Medium  priors_count_(-0.038, 7.6]  priors_count_(7.6, 15.2]  \\\n",
       "0                     0                           1                         0   \n",
       "1                     0                           1                         0   \n",
       "2                     0                           1                         0   \n",
       "5                     0                           1                         0   \n",
       "6                     1                           0                         1   \n",
       "...                 ...                         ...                       ...   \n",
       "7209                  1                           1                         0   \n",
       "7210                  0                           1                         0   \n",
       "7211                  0                           1                         0   \n",
       "7212                  0                           1                         0   \n",
       "7213                  0                           1                         0   \n",
       "\n",
       "      priors_count_(15.2, 22.8]  priors_count_(22.8, 30.4]  \\\n",
       "0                             0                          0   \n",
       "1                             0                          0   \n",
       "2                             0                          0   \n",
       "5                             0                          0   \n",
       "6                             0                          0   \n",
       "...                         ...                        ...   \n",
       "7209                          0                          0   \n",
       "7210                          0                          0   \n",
       "7211                          0                          0   \n",
       "7212                          0                          0   \n",
       "7213                          0                          0   \n",
       "\n",
       "      priors_count_(30.4, 38.0]  ...  in_custody_2009  in_custody_2013  \\\n",
       "0                             0  ...                0                0   \n",
       "1                             0  ...                0                1   \n",
       "2                             0  ...                0                1   \n",
       "5                             0  ...                0                1   \n",
       "6                             0  ...                0                0   \n",
       "...                         ...  ...              ...              ...   \n",
       "7209                          0  ...                0                1   \n",
       "7210                          0  ...                0                0   \n",
       "7211                          0  ...                0                0   \n",
       "7212                          0  ...                0                0   \n",
       "7213                          0  ...                0                0   \n",
       "\n",
       "      in_custody_2014  in_custody_2015  in_custody_2016  out_custody_2013  \\\n",
       "0                   1                0                0                 0   \n",
       "1                   0                0                0                 1   \n",
       "2                   0                0                0                 1   \n",
       "5                   0                0                0                 1   \n",
       "6                   1                0                0                 0   \n",
       "...               ...              ...              ...               ...   \n",
       "7209                0                0                0                 1   \n",
       "7210                1                0                0                 0   \n",
       "7211                1                0                0                 0   \n",
       "7212                1                0                0                 0   \n",
       "7213                0                1                0                 0   \n",
       "\n",
       "      out_custody_2014  out_custody_2015  out_custody_2016  out_custody_2020  \n",
       "0                    1                 0                 0                 0  \n",
       "1                    0                 0                 0                 0  \n",
       "2                    0                 0                 0                 0  \n",
       "5                    0                 0                 0                 0  \n",
       "6                    1                 0                 0                 0  \n",
       "...                ...               ...               ...               ...  \n",
       "7209                 0                 0                 0                 0  \n",
       "7210                 1                 0                 0                 0  \n",
       "7211                 1                 0                 0                 0  \n",
       "7212                 1                 0                 0                 0  \n",
       "7213                 0                 1                 0                 0  \n",
       "\n",
       "[6172 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to one-hot-encoding\n",
    "compas_cat_X = pd.get_dummies(compas_X, columns=compas_X.columns)\n",
    "compas_cat_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b33edc1b-9093-4059-b422-fd7d60e99b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a train and test split with same proportional size as Adult dataset \n",
    "adult_test_size = 1/3\n",
    "compas_train_cat_X, compas_test_cat_X, compas_train_y, compas_test_y= train_test_split(compas_cat_X, compas_y, test_size=adult_test_size, random_state=42)\n",
    "\n",
    "# also for the sensitive attributes with same random_state\n",
    "compas_train_sex, compas_test_sex, compas_train_y, compas_test_y = train_test_split(compas_sex, compas_y, test_size=adult_test_size, random_state=42)\n",
    "compas_train_race, compas_test_race, compas_train_y, compas_test_y = train_test_split(compas_race, compas_y, test_size=adult_test_size, random_state=42)\n",
    "compas_train_age, compas_test_age, compas_train_y, compas_test_y = train_test_split(compas_age, compas_y, test_size=adult_test_size, random_state=42)\n",
    "compas_train_sex_race, compas_test_sex_race, compas_train_y, compas_test_y = train_test_split(compas_sex_race, compas_y, test_size=adult_test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05712f2f-0e91-48c1-b985-d6d2a3ea4395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reindex all datasets, drop=True to prevent addition of \"index\" column\n",
    "compas_train_cat_X = compas_train_cat_X.reset_index(drop=True)\n",
    "compas_train_y = compas_train_y.reset_index(drop=True)\n",
    "compas_train_sex = compas_train_sex.reset_index(drop=True)\n",
    "compas_train_race = compas_train_race.reset_index(drop=True)\n",
    "compas_train_sex_race = compas_train_sex_race.reset_index(drop=True)\n",
    "compas_train_age = compas_train_age.reset_index(drop=True)\n",
    "\n",
    "compas_test_cat_X = compas_test_cat_X.reset_index(drop=True)\n",
    "compas_test_y = compas_test_y.reset_index(drop=True)\n",
    "compas_test_sex = compas_test_sex.reset_index(drop=True)\n",
    "compas_test_race = compas_test_race.reset_index(drop=True)\n",
    "compas_test_sex_race = compas_test_sex_race.reset_index(drop=True)\n",
    "compas_test_age = compas_test_age.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1dd54d-b415-4f8b-88d8-ce274fe30b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PAFER\n",
    "The code that implements the PAFER algorithm as found in Algorithm 1 in the paper https://arxiv.org/abs/2312.08413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b4d259-5c3b-4735-9c9d-0a6f1a41ad4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def oracle(dataset, sens_dataset, rule, s_i, mechanism=None, epsilon=0.05, delta=0.001):\n",
    "    \"\"\"Returns some (differentially privatised) statistics on the sensitive attribute for the specified dataframe and rule.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The DataFrame that the developers own, which does not contain sensitive attributes.\n",
    "            Used to calculate total quantities in (root) nodes.\n",
    "        sens_dataset: A Series that the developers do not own, which contains the sensitive attributes. \n",
    "            Combined sensitive attributes should be encoded as a Series, e.g. Black-Female\n",
    "        rule: The rule for which the to estimate the sensitive attribute. \n",
    "            rule must be a pandas conditional expression as a string, e.g. \"(adult_test_cat_X['marital-status_Married-civ-spouse']<= 0.5)\"\n",
    "        s_i: The sensitive attribute, its name comes from the ith element in the set S of sensitive attributes.\n",
    "            s_i should thus be in sens_dataset. \n",
    "        mechanism: The privacy mechanism used on the returned counts. Can be one of \"gaussian\", \"laplacian\", \"exponential\", None. \n",
    "        epsilon: The privacy budget. Should be larger than 0.\n",
    "        delta: The privacy margin. Ignored when mechanism is either laplacian or gaussian. Should be in (0, 1]. \n",
    "        \n",
    "    Returns:\n",
    "        The number of times s_i occurs in sens_dataset, potentially privatised via the mechanism. \n",
    "        \"\"\"\n",
    "        \n",
    "    # check epsilon and delta parameters\n",
    "    if epsilon <= 0 or (mechanism == \"gaussian\" and (delta <= 0 or delta > 1 or epsilon > 1)):\n",
    "        raise ValueError(\"The value of delta should be in (0,1] when using the gaussian mechanism\")\n",
    "    \n",
    "    if not sens_dataset.isin([s_i]).any():\n",
    "        raise KeyError(\"The requested sensitive attribute (s_i) is not in the sensitive dataframe (sens_dataset)\")\n",
    "        \n",
    "    # the answer if no privacy mechanism is applied\n",
    "    try:\n",
    "        # engine might differ for your version, i.e. engine=\"pandas\"\n",
    "        no_mechanism = sens_dataset.loc[dataset[pd.eval(rule, engine='python')].index].value_counts(sort=False)[s_i]\n",
    "        \n",
    "    except KeyError:\n",
    "        no_mechanism = 0\n",
    "    \n",
    "    if mechanism == \"laplacian\":\n",
    "        # this is a histogram query so the l1-sensitivity = 1 as per Dwork & Roth \n",
    "        sensitivity = 1\n",
    "        return no_mechanism + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "    \n",
    "    elif mechanism == \"gaussian\":\n",
    "        # this is a histogram query so the l2-sensitivity = 2 as per Dwork & Roth\n",
    "        sensitivity = 2\n",
    "        return no_mechanism + np.random.normal(loc=0, scale=2 * sensitivity**2 * np.log(1.25 / delta) / epsilon**2)\n",
    "    \n",
    "    elif mechanism == \"exponential\":\n",
    "        # this query can only change by 1 if an instance is omitted so l1-sensitivity = 1\n",
    "        sensitivity = 1\n",
    "        \n",
    "        # np.arange is [start, stop) so + 1 for entire possible range\n",
    "        possible_values = np.arange(0, sens_dataset.loc[dataset[pd.eval(rule, engine='python')].index].value_counts().to_numpy().sum() + 1)\n",
    "        \n",
    "        # the utility is higher when the value is closer to the actual value\n",
    "        utility_scores = np.array([no_mechanism - abs(no_mechanism - value) for value in possible_values]) / 100\n",
    "        probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in utility_scores]\n",
    "        \n",
    "        # normalize probabilties to sum to 1\n",
    "        probabilities /= np.linalg.norm(probabilities, ord=1)\n",
    "        return np.random.choice(possible_values, p=probabilities)\n",
    "\n",
    "    # if no mechanism is given, return the unprivatised cocunt\n",
    "    return no_mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505f2212-a1d9-44b4-9cc5-49815eb5756c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def statistical_parity(y_pred, sens_dataset):\n",
    "    \"\"\"Calculates Statistical Parity Ratio using the predictions and the actual sensitive feature values. \n",
    "    \n",
    "    Args:\n",
    "        y_pred: The predictions, should be of same size as sens_dataset.\n",
    "        sens_dataset: The Series with the sensitive attributes.\n",
    "        \n",
    "    Returns:\n",
    "        The true statistical parity ratio.\n",
    "        \"\"\"\n",
    "    accept_rates = []\n",
    "    \n",
    "    for sens_attr in sorted(sens_dataset.unique()):\n",
    "        accept_rates.append(np.sum((sens_dataset == sens_attr) & y_pred) / np.sum(sens_dataset == sens_attr))\n",
    "        \n",
    "    return min(accept_rates) / max(accept_rates)\n",
    "\n",
    "\n",
    "def estimate_sp(pos_ruleset, dataset, sens_dataset, S, mechanism, epsilon, delta=0.001):\n",
    "    \"\"\"Returns the estimated Statistical Parity of a tree for a privacy mechanism. The PAFER algorithm. \n",
    "    \n",
    "    Args:\n",
    "        pos_ruleset: A list of rules that classify favorably in the tree. This is the representation of the\n",
    "        (relevant parts of the) tree. \n",
    "        dataset: The DataFrame that the developers own that does not contain sensitive feature values.\n",
    "        sens_dataset: The Series that contains the sensitive features, which the developers do not own.\n",
    "        S: The set/list of sensitive attributes, should all be in the sens_dataset attribute.\n",
    "        mechanism: The mechanism with which to privatise the query answers. \n",
    "        epsilon: The privacy budget for the privacy mechanism. Should be larger than 0.\n",
    "        delta: The privacy margin. Ignored when mechanism is either laplacian or gaussian. Should be in (0, 1].\n",
    "        \n",
    "    Returns:\n",
    "        The statistical parity ratio for the specified pos_ruleset. \n",
    "        \"\"\"\n",
    "    \n",
    "    poscounts_per_si = np.zeros(len(S))\n",
    "    \n",
    "    # the variable name of the current dataset is inferred from the ruleset\n",
    "    datasetname = str(pos_ruleset[0].split('[')[0])[1:]\n",
    "    \n",
    "    # the base rule is a rule that includes all individuals, i.e. the condition is a tautology\n",
    "    # in this case we select all rows that have a value that is in the set of possible values of the first column\n",
    "    base_rule = f\"({datasetname}[{datasetname}.columns[0]].isin({datasetname}[{datasetname}.columns[0]].unique()))\"\n",
    "    \n",
    "    # query the size of each sensitive attribute in the dataset\n",
    "    total_per_si = [oracle(dataset, sens_dataset, base_rule, s_i, mechanism, 0.5 * epsilon, delta) for s_i in S]\n",
    "    \n",
    "    # replace each invalid value with balanced totals\n",
    "    for i, tot in enumerate(total_per_si):\n",
    "        if tot < 0 or tot > len(sens_dataset):\n",
    "            total_per_si[i] = (1 / len(S)) * len(sens_dataset)\n",
    "        \n",
    "    total_per_si = np.array(total_per_si)\n",
    "    \n",
    "    for rule in pos_ruleset:\n",
    "        # for each rule we find the distribution of sensitive attributes\n",
    "        rule_counts = np.zeros(len(S))\n",
    "        rule_total = len(sens_dataset[pd.eval(rule)])\n",
    "        \n",
    "        for i, s_i in enumerate(S):\n",
    "            # because the queries are disjoint, epsilon remains equal across queries\n",
    "            answer = round(oracle(dataset, sens_dataset, rule, s_i, mechanism, 0.5 * epsilon, delta))\n",
    "\n",
    "            # if invalid answers from query: replace with balanced node value\n",
    "            if answer < 0 or answer > len(sens_dataset):\n",
    "                answer = (1 / len(S)) * rule_total\n",
    "\n",
    "            rule_counts[i] += answer\n",
    "        \n",
    "        # the distribution for the current rule is added to the total\n",
    "        poscounts_per_si += rule_counts\n",
    "    \n",
    "    # calculate and return sp\n",
    "    accept_rates = poscounts_per_si / total_per_si\n",
    "    return np.min(accept_rates) / np.max(accept_rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668f072-f760-406d-a3d0-0fdcde2d79a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tree construction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c8d289e-0527-4240-96f2-074cfb35eace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_tree(dataset, dataset_labels, minleaf=1, ccp_alpha=0.0):\n",
    "    \"\"\"Train a tree for balanced accuracy performance using grid search. \n",
    "    \n",
    "    dataset: The training data (without sensitive attributes!).\n",
    "    dataset_labels: The true outcomes for the prediction task.\n",
    "    minleaf: The tree construction parameter denoting the minimum number of instances in a leaf node.\n",
    "        minleaf should be in (0, 1]. \n",
    "        \n",
    "    Returns: \n",
    "        The best performing decision tree.\"\"\"\n",
    "    \n",
    "    # no random_state because we want a different tree each run\n",
    "    tree = DecisionTreeClassifier()\n",
    "\n",
    "    parameter_grid = {\"criterion\":[\"entropy\", \"gini\"],\n",
    "                      \"max_features\":[\"sqrt\", \"log2\"], \n",
    "                      \"min_samples_leaf\":[minleaf], \"ccp_alpha\": [ccp_alpha]}\n",
    "    \n",
    "    # train and return the best tree\n",
    "    tree_cv = GridSearchCV(tree, param_grid=parameter_grid, scoring='balanced_accuracy', n_jobs=2, cv=3, verbose=0)\n",
    "    tree_cv.fit(dataset, dataset_labels)\n",
    "    best_tree = tree_cv.best_estimator_\n",
    "    return best_tree\n",
    "\n",
    "best_tree = find_best_tree(compas_train_cat_X, compas_train_y, ccp_alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "311e2661-0593-4cda-b3d3-6ec26b5be06a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taken from: https://stackoverflow.com/a/51398390\n",
    "def is_leaf(inner_tree, index):\n",
    "    # check whether node is leaf node\n",
    "    return (inner_tree.children_left[index] == TREE_LEAF and \n",
    "            inner_tree.children_right[index] == TREE_LEAF)\n",
    "\n",
    "def prune_index(inner_tree, decisions, index=0):\n",
    "    # start pruning from the bottom - if we start from the top, we might miss\n",
    "    # nodes that become leaves during pruning\n",
    "    if not is_leaf(inner_tree, inner_tree.children_left[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_left[index])\n",
    "    if not is_leaf(inner_tree, inner_tree.children_right[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_right[index])\n",
    "\n",
    "    # prune children if both children are leaves now and make the same decision\n",
    "    if (is_leaf(inner_tree, inner_tree.children_left[index]) and\n",
    "        is_leaf(inner_tree, inner_tree.children_right[index]) and\n",
    "        (decisions[index] == decisions[inner_tree.children_left[index]]) and \n",
    "        (decisions[index] == decisions[inner_tree.children_right[index]])):\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "\n",
    "def prune_duplicate_leaves(mdl):\n",
    "    # Remove leaves if all siblings make the same decision\n",
    "    decisions = mdl.tree_.value.argmax(axis=2).flatten().tolist() # Decision for each node\n",
    "    prune_index(mdl.tree_, decisions)\n",
    "    \n",
    "# pruning happens in-place\n",
    "prune_duplicate_leaves(best_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0aa8b8ba-acea-4432-a13e-02e0f4f38994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positive_rules (tree, rules):\n",
    "    \"\"\"From the extracted rules, return those that have a favorable classification. \n",
    "\n",
    "    Arg:\n",
    "        tree: The tree classification object from which the rules are extracted. \n",
    "        rules: Dict of which the values are rule strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of all the rules that classify favorably\"\"\"\n",
    "\n",
    "    # only those rules are added for which the majority of individuals in the node is at index 1, i.e. max\n",
    "    # index 1 corresponds to class 1 which we ensured was the favorable outcome\n",
    "    return [rule for node_id, rule in rules.items() if np.argmax(tree.tree_.value[node_id][0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af57931f-a5db-4172-859d-42fae59a22ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['()']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taken from: https://stackoverflow.com/a/56427596\n",
    "def extract_pos_rules(tree, dataset, datasetname):\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    feature = tree.tree_.feature\n",
    "    threshold = tree.tree_.threshold\n",
    "\n",
    "    def find_path(node_numb, path, x):\n",
    "        path.append(node_numb)\n",
    "        if node_numb == x:\n",
    "            return True\n",
    "        left = False\n",
    "        right = False\n",
    "        if (children_left[node_numb] !=-1):\n",
    "            left = find_path(children_left[node_numb], path, x)\n",
    "        if (children_right[node_numb] !=-1):\n",
    "            right = find_path(children_right[node_numb], path, x)\n",
    "        if left or right :\n",
    "            return True\n",
    "        path.remove(node_numb)\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_rule(datasetname, path, column_names):\n",
    "        mask = '('\n",
    "        for index, node in enumerate(path):\n",
    "            # check if we are not in the leaf\n",
    "            if index!=len(path)-1:\n",
    "                # under or over the threshold?\n",
    "                if (children_left[node] == path[index+1]):\n",
    "                    mask += f\"{datasetname}['{column_names[feature[node]]}']<= {threshold[node]}\\t \"\n",
    "                else:\n",
    "                    mask += f\"{datasetname}['{column_names[feature[node]]}']> {threshold[node]} \\t \"\n",
    "\n",
    "        # insert the & at the right places\n",
    "        mask = mask.replace(\"\\t\", \"&\", mask.count(\"\\t\") - 1)\n",
    "        mask = mask.replace(\"\\t\", \"\")\n",
    "        mask += \")\"\n",
    "        return mask\n",
    "    \n",
    "    # Leaves\n",
    "    leave_id = tree.apply(dataset)\n",
    "\n",
    "    paths = {}\n",
    "    for leaf in np.unique(leave_id):\n",
    "        path_leaf = []\n",
    "        find_path(0, path_leaf, leaf)\n",
    "        paths[leaf] = np.unique(np.sort(path_leaf))\n",
    "\n",
    "    rules = {}\n",
    "    for key in paths:\n",
    "        rules[key] = get_rule(datasetname, paths[key], [name for name in dataset.columns])\n",
    "        \n",
    "    return positive_rules(tree, rules)\n",
    "        \n",
    "extract_pos_rules(best_tree, compas_train_cat_X, \"compas_train_cat_X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e6cb6-a3eb-4213-a9a2-9ac977ce6449",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87064d71-b130-4e39-8a2e-417dc8838437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootstrap(dataset, dataset_labels, sens_dataset):\n",
    "    \"\"\"A bootstrapping function that helps to diversify the tree generating process.\"\"\"\n",
    "    indices = np.random.choice(dataset.index, size=len(dataset.index))\n",
    "    \n",
    "    return dataset.iloc[indices], dataset_labels.iloc[indices], sens_dataset.iloc[indices]\n",
    "\n",
    "# dataset, labels, sens_dataset = bootstrap(compas_test_cat_X, compas_test_y, compas_test_sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4122c8a-e29b-425d-94d1-7ce76285a373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experiment(trainset, sens_trainset, trainsetname, trainset_labels, testset, sens_testset, testsetname, \n",
    "               testset_labels, epsilons=[0.05, 0.1, 0.15, 0.2, 0.25], minleaf=1, ccp_alpha=0.0, runs=5, combined=False):\n",
    "    \"\"\"Performs an experiment as described in the paper.\n",
    "    \n",
    "    trainset: The DataFrame containing the training instances.\n",
    "    sens_trainset: The Series containing the sensitive attribute values for the trainset.\n",
    "    trainsetname: The variable name of the trainset. Required because of rule evaluation.\n",
    "    trainset_labels: The true outcomes for the prediction task for the trainset.\n",
    "    testset: The DataFrame containing the test instances.\n",
    "    sens_testset: The Series containing the sensitive attribute values for the testset.\n",
    "    testsetname: The variable name of the testset. Required because of rule evaluation.\n",
    "    testset_labels: The true outcomes for the prediction task for the testset.\n",
    "    epsilons: The different privacy budgets to try. \n",
    "        epsilons must be a list.\n",
    "    minleaf: The tree construction parameter denoting the minimum number of instances in a leaf node.\n",
    "        minleaf should be in (0, 1].\n",
    "    runs: The number of runs to average over. Advised to be quite high (e.g. 50) to compensate for noise.\n",
    "    combined: Whether to combine all positive rules into one query. \n",
    "    \n",
    "    Returns:\n",
    "        The true SP of the tree and the estimated SP.\"\"\"\n",
    "    \n",
    "    \n",
    "    tree_sps = np.zeros((runs, len(epsilons)))\n",
    "    tree_depths = np.zeros((runs, len(epsilons)))\n",
    "    estimated_sps = np.zeros((runs, len(epsilons)))\n",
    "    for i in range(runs):\n",
    "        ruleset = []\n",
    "        \n",
    "        # keep boostrapping until we find a ruleset that has at least one positive rule\n",
    "        while ruleset == [] or ruleset == ['()']:\n",
    "            # sample with replacement\n",
    "            dataset, dataset_labels, sens_dataset = bootstrap(trainset, trainset_labels, sens_trainset)\n",
    "            \n",
    "            # build tree \n",
    "            best_tree = find_best_tree(trainset, trainset_labels, minleaf, ccp_alpha)\n",
    "        \n",
    "            # extract positive rules\n",
    "            prune_duplicate_leaves(best_tree)\n",
    "            ruleset = extract_pos_rules(best_tree, trainset, testsetname)\n",
    "        \n",
    "        if combined:\n",
    "            ruleset = [\" | \".join(rule for rule in ruleset)]\n",
    "\n",
    "        # calculate true SP\n",
    "        tree_sps[i] = statistical_parity(best_tree.predict(testset), sens_testset)\n",
    "        tree_depths[i] = best_tree.tree_.max_depth\n",
    "        \n",
    "        # apply PAFER\n",
    "        for j, epsilon in enumerate(epsilons):\n",
    "            estimated_sps[i, j] = estimate_sp(ruleset, testset, sens_testset, sorted(sens_testset.unique()), mechanism='laplacian', epsilon=epsilon)\n",
    "        \n",
    "    return tree_sps, tree_depths, estimated_sps\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698ed27-fa42-42a7-92d0-2e4f865e8818",
   "metadata": {},
   "source": [
    "## MINLEAF EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e60da-4040-49cf-8590-58ae33233ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to run the experiments\n",
    "# TODO: current datastructures are not optimal or intuitive so could be helpful to streamline (for plotting)\n",
    "minleafs = np.linspace(0.2, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for minleaf in tqdm(minleafs):\n",
    "    t_sps, t_depths, e_sps = experiment(compas_train_cat_X, compas_train_sex, \"compas_train_cat_X\", compas_train_y, \n",
    "                                         compas_test_cat_X, compas_test_sex, \"compas_test_cat_X\", compas_test_y, minleaf=minleaf, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18205c30-d9e8-46aa-ac6c-919cce766858",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"compas-sex-minleaf-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0f9e0-7ed1-480b-82c4-23dfd3f84a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "minleafs = np.linspace(0.2, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for minleaf in tqdm(minleafs):\n",
    "    t_sps, t_depths, e_sps = experiment(compas_train_cat_X, compas_train_race, \"compas_train_cat_X\", compas_train_y, \n",
    "                                         compas_test_cat_X, compas_test_race, \"compas_test_cat_X\", compas_test_y, minleaf=minleaf, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad234a5-5c13-4c19-839a-06fd2bfd4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"compas-race-minleaf-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f029fbe-0ca2-4145-9f12-60dba5099fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minleafs = np.linspace(0.2, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for minleaf in tqdm(minleafs):\n",
    "    t_sps, t_depths, e_sps = experiment(compas_train_cat_X, compas_train_sex_race, \"compas_train_cat_X\", compas_train_y, \n",
    "                                         compas_test_cat_X, compas_test_sex_race, \"compas_test_cat_X\", compas_test_y, minleaf=minleaf, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec104873-9c08-47f5-a113-340dc71ffc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"compas-sex_race-minleaf-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ce345-a70a-41e3-a0de-debbd4f91fab",
   "metadata": {},
   "source": [
    "## CCP_ALPHA EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9a288ef-b77a-42e8-934c-ca5ebc537432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 10/10 [11:04<00:00, 66.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# script to run the experiments\n",
    "# TODO: current datastructures are not optimal or intuitive so could be helpful to streamline (for plotting)\n",
    "ccp_alphas = np.linspace(0.05, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for ccp_alpha in tqdm(ccp_alphas):\n",
    "    t_sps, t_depths, e_sps = experiment(compas_train_cat_X, compas_train_sex, \"compas_train_cat_X\", compas_train_y, \n",
    "                                         compas_test_cat_X, compas_test_sex, \"compas_test_cat_X\", compas_test_y, ccp_alpha=ccp_alpha, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f40470ef-af86-44f9-af0e-1a8563fd71cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"compas-sex-ccp-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b1fcac5-0597-4db7-bcbe-a10edbbc7f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2        0.19748101 0.19496203 0.19244304 0.18992405 0.18740506\n",
      " 0.18488608 0.18236709 0.1798481  0.17732911 0.17481013 0.17229114\n",
      " 0.16977215 0.16725316 0.16473418 0.16221519 0.1596962  0.15717722\n",
      " 0.15465823 0.15213924 0.14962025 0.14710127 0.14458228 0.14206329\n",
      " 0.1395443  0.13702532 0.13450633 0.13198734 0.12946835 0.12694937\n",
      " 0.12443038 0.12191139 0.11939241 0.11687342 0.11435443 0.11183544\n",
      " 0.10931646 0.10679747 0.10427848 0.10175949 0.09924051 0.09672152\n",
      " 0.09420253 0.09168354 0.08916456 0.08664557 0.08412658 0.08160759\n",
      " 0.07908861 0.07656962 0.07405063 0.07153165 0.06901266 0.06649367\n",
      " 0.06397468 0.0614557  0.05893671 0.05641772 0.05389873 0.05137975\n",
      " 0.04886076 0.04634177 0.04382278 0.0413038  0.03878481 0.03626582\n",
      " 0.03374684 0.03122785 0.02870886 0.02618987 0.02367089 0.0211519\n",
      " 0.01863291 0.01611392 0.01359494 0.01107595 0.00855696 0.00603797\n",
      " 0.00351899 0.001     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 80/80 [7:30:14<00:00, 337.68s/it]\n"
     ]
    }
   ],
   "source": [
    "ccp_alphas = np.linspace(0.05, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for ccp_alpha in tqdm(ccp_alphas):\n",
    "    t_sps, t_depths, e_sps = experiment(compas_train_cat_X, compas_train_race, \"compas_train_cat_X\", compas_train_y, \n",
    "                                         compas_test_cat_X, compas_test_race, \"compas_test_cat_X\", compas_test_y, ccp_alpha=ccp_alpha, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18ea14-ac2c-4814-8be5-6687ec461dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"compas-race-ccp-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc869888-922f-43a8-adf3-d51c2dd202e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ccp_alphas = np.linspace(0.05, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for ccp_alpha in tqdm(ccp_alphas):\n",
    "    t_sps, t_depths, e_sps = experiment(compas_train_cat_X, compas_train_sex_race, \"compas_train_cat_X\", compas_train_y, \n",
    "                                         compas_test_cat_X, compas_test_sex_race, \"compas_test_cat_X\", compas_test_y, ccp_alpha=ccp_alpha, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26f3047f-a007-4081-a557-8762821490e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"compas-sex_race-ccp-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea887d5f-9907-4bbd-aafb-94dd58c45393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
