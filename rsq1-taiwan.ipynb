{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb44b8b4-2fa8-400a-9ac7-cfb82b0912d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b7dbab-c047-4f86-b87b-cfca3bc44d02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>220000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>88004</td>\n",
       "      <td>31237</td>\n",
       "      <td>15980</td>\n",
       "      <td>8500</td>\n",
       "      <td>20000</td>\n",
       "      <td>5003</td>\n",
       "      <td>3047</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>8979</td>\n",
       "      <td>5190</td>\n",
       "      <td>0</td>\n",
       "      <td>1837</td>\n",
       "      <td>3526</td>\n",
       "      <td>8998</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>20878</td>\n",
       "      <td>20582</td>\n",
       "      <td>19357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22000</td>\n",
       "      <td>4200</td>\n",
       "      <td>2000</td>\n",
       "      <td>3100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>80000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52774</td>\n",
       "      <td>11855</td>\n",
       "      <td>48944</td>\n",
       "      <td>85900</td>\n",
       "      <td>3409</td>\n",
       "      <td>1178</td>\n",
       "      <td>1926</td>\n",
       "      <td>52964</td>\n",
       "      <td>1804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36535</td>\n",
       "      <td>32428</td>\n",
       "      <td>15313</td>\n",
       "      <td>2078</td>\n",
       "      <td>1800</td>\n",
       "      <td>1430</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "0          1      20000    2          2         1   24      2      2     -1   \n",
       "1          2     120000    2          2         2   26     -1      2      0   \n",
       "2          3      90000    2          2         2   34      0      0      0   \n",
       "3          4      50000    2          2         1   37      0      0      0   \n",
       "4          5      50000    1          2         1   57     -1      0     -1   \n",
       "...      ...        ...  ...        ...       ...  ...    ...    ...    ...   \n",
       "29995  29996     220000    1          3         1   39      0      0      0   \n",
       "29996  29997     150000    1          3         2   43     -1     -1     -1   \n",
       "29997  29998      30000    1          2         2   37      4      3      2   \n",
       "29998  29999      80000    1          3         1   41      1     -1      0   \n",
       "29999  30000      50000    1          2         1   46      0      0      0   \n",
       "\n",
       "       PAY_4  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0         -1  ...          0          0          0         0       689   \n",
       "1          0  ...       3272       3455       3261         0      1000   \n",
       "2          0  ...      14331      14948      15549      1518      1500   \n",
       "3          0  ...      28314      28959      29547      2000      2019   \n",
       "4          0  ...      20940      19146      19131      2000     36681   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29995      0  ...      88004      31237      15980      8500     20000   \n",
       "29996     -1  ...       8979       5190          0      1837      3526   \n",
       "29997     -1  ...      20878      20582      19357         0         0   \n",
       "29998      0  ...      52774      11855      48944     85900      3409   \n",
       "29999      0  ...      36535      32428      15313      2078      1800   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0             0         0         0         0                           1  \n",
       "1          1000      1000         0      2000                           1  \n",
       "2          1000      1000      1000      5000                           0  \n",
       "3          1200      1100      1069      1000                           0  \n",
       "4         10000      9000       689       679                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29995      5003      3047      5000      1000                           0  \n",
       "29996      8998       129         0         0                           0  \n",
       "29997     22000      4200      2000      3100                           1  \n",
       "29998      1178      1926     52964      1804                           1  \n",
       "29999      1430      1000      1000      1000                           1  \n",
       "\n",
       "[30000 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taiwan_df = pd.read_csv('taiwan-original.csv')\n",
    "taiwan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4729e87-d65a-4e76-91f9-4960b3e26dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop ID row as it is meaningless\n",
    "taiwan_df = taiwan_df.drop(columns=[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e74129-271e-4e5f-a6c9-4a829dd3aef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate labels\n",
    "taiwan_y = taiwan_df[\"default payment next month\"]\n",
    "taiwan_X = taiwan_df.drop(\"default payment next month\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94f0c83-c1c7-4802-a2c5-7e6d89c02fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taiwan_age = taiwan_X[\"AGE\"]\n",
    "taiwan_sex = taiwan_X[\"SEX\"]\n",
    "taiwan_X = taiwan_X.drop(columns=[\"AGE\", \"SEX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22cf8ad5-3db9-45ab-9203-00be0c937ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a train and test split with same proportional size as Adult dataset \n",
    "adult_test_size = 1/3\n",
    "taiwan_train_X, taiwan_test_X, taiwan_train_y, taiwan_test_y = train_test_split(taiwan_X, taiwan_y, test_size=adult_test_size, random_state=42)\n",
    "\n",
    "# also for the sensitive attributes with same random_state\n",
    "taiwan_train_sex, taiwan_test_sex, taiwan_train_y, taiwan_test_y = train_test_split(taiwan_sex, taiwan_y, test_size=adult_test_size, random_state=42)\n",
    "taiwan_train_age, taiwan_test_age, taiwan_train_y, taiwan_test_y = train_test_split(taiwan_age, taiwan_y, test_size=adult_test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efa275a7-472a-4499-8033-830e8a1c6652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reindex all datasets\n",
    "taiwan_train_X = taiwan_train_X.reset_index(drop=True)\n",
    "taiwan_train_y = taiwan_train_y.reset_index(drop=True)\n",
    "taiwan_train_sex = taiwan_train_sex.reset_index(drop=True)\n",
    "taiwan_train_age = taiwan_train_age.reset_index(drop=True)\n",
    "\n",
    "taiwan_test_X = taiwan_test_X.reset_index(drop=True)\n",
    "taiwan_test_y = taiwan_test_y.reset_index(drop=True)\n",
    "taiwan_test_sex = taiwan_test_sex.reset_index(drop=True)\n",
    "taiwan_test_age = taiwan_test_age.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a861612-93f3-42e5-8d73-78965a627a14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PAFER\n",
    "The code that implements the PAFER algorithm as found in Algorithm 1 in the paper https://arxiv.org/abs/2312.08413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52febb05-8ef8-4a96-a4d9-c973c434b191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def oracle(dataset, sens_dataset, rule, s_i, mechanism=None, epsilon=0.05, delta=0.001):\n",
    "    \"\"\"Returns some (differentially privatised) statistics on the sensitive attribute for the specified dataframe and rule.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The DataFrame that the developers own, which does not contain sensitive attributes.\n",
    "            Used to calculate total quantities in (root) nodes.\n",
    "        sens_dataset: A Series that the developers do not own, which contains the sensitive attributes. \n",
    "            Combined sensitive attributes should be encoded as a Series, e.g. Black-Female\n",
    "        rule: The rule for which the to estimate the sensitive attribute. \n",
    "            rule must be a pandas conditional expression as a string, e.g. \"(adult_test_cat_X['marital-status_Married-civ-spouse']<= 0.5)\"\n",
    "        s_i: The sensitive attribute, its name comes from the ith element in the set S of sensitive attributes.\n",
    "            s_i should thus be in sens_dataset. \n",
    "        mechanism: The privacy mechanism used on the returned counts. Can be one of \"gaussian\", \"laplacian\", \"exponential\", None. \n",
    "        epsilon: The privacy budget. Should be larger than 0.\n",
    "        delta: The privacy margin. Ignored when mechanism is either laplacian or gaussian. Should be in (0, 1]. \n",
    "        \n",
    "    Returns:\n",
    "        The number of times s_i occurs in sens_dataset, potentially privatised via the mechanism. \n",
    "        \"\"\"\n",
    "        \n",
    "    # check epsilon and delta parameters\n",
    "    if epsilon <= 0 or (mechanism == \"gaussian\" and (delta <= 0 or delta > 1 or epsilon > 1)):\n",
    "        raise ValueError(\"The value of delta should be in (0,1] when using the gaussian mechanism\")\n",
    "    \n",
    "    if not sens_dataset.isin([s_i]).any():\n",
    "        raise KeyError(\"The requested sensitive attribute (s_i) is not in the sensitive dataframe (sens_dataset)\")\n",
    "        \n",
    "    # the answer if no privacy mechanism is applied\n",
    "    try:\n",
    "        # engine might differ for your version, i.e. engine=\"pandas\"\n",
    "        no_mechanism = sens_dataset.loc[dataset[pd.eval(rule, engine='python')].index].value_counts(sort=False)[s_i]\n",
    "        \n",
    "    except KeyError:\n",
    "        no_mechanism = 0\n",
    "    \n",
    "    if mechanism == \"laplacian\":\n",
    "        # this is a histogram query so the l1-sensitivity = 1 as per Dwork & Roth \n",
    "        sensitivity = 1\n",
    "        return no_mechanism + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "    \n",
    "    elif mechanism == \"gaussian\":\n",
    "        # this is a histogram query so the l2-sensitivity = 2 as per Dwork & Roth\n",
    "        sensitivity = 2\n",
    "        return no_mechanism + np.random.normal(loc=0, scale=2 * sensitivity**2 * np.log(1.25 / delta) / epsilon**2)\n",
    "    \n",
    "    elif mechanism == \"exponential\":\n",
    "        # this query can only change by 1 if an instance is omitted so l1-sensitivity = 1\n",
    "        sensitivity = 1\n",
    "        \n",
    "        # np.arange is [start, stop) so + 1 for entire possible range\n",
    "        possible_values = np.arange(0, sens_dataset.loc[dataset[pd.eval(rule, engine='python')].index].value_counts().to_numpy().sum() + 1)\n",
    "        \n",
    "        # the utility is higher when the value is closer to the actual value\n",
    "        utility_scores = np.array([no_mechanism - abs(no_mechanism - value) for value in possible_values]) / 100\n",
    "        probabilities = [np.exp(epsilon * score / (2 * sensitivity)) for score in utility_scores]\n",
    "        \n",
    "        # normalize probabilties to sum to 1\n",
    "        probabilities /= np.linalg.norm(probabilities, ord=1)\n",
    "        return np.random.choice(possible_values, p=probabilities)\n",
    "\n",
    "    # if no mechanism is given, return the unprivatised cocunt\n",
    "    return no_mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6d071ca-dd44-45cf-86ca-f3de0f12e738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def statistical_parity(y_pred, sens_dataset):\n",
    "    \"\"\"Calculates Statistical Parity Ratio using the predictions and the actual sensitive feature values. \n",
    "    \n",
    "    Args:\n",
    "        y_pred: The predictions, should be of same size as sens_dataset.\n",
    "        sens_dataset: The Series with the sensitive attributes.\n",
    "        \n",
    "    Returns:\n",
    "        The true statistical parity ratio.\n",
    "        \"\"\"\n",
    "    accept_rates = []\n",
    "    \n",
    "    for sens_attr in sorted(sens_dataset.unique()):\n",
    "        accept_rates.append(np.sum((sens_dataset == sens_attr) & y_pred) / np.sum(sens_dataset == sens_attr))\n",
    "        \n",
    "    return min(accept_rates) / max(accept_rates)\n",
    "\n",
    "\n",
    "def estimate_sp(pos_ruleset, dataset, sens_dataset, S, mechanism, epsilon, delta=0.001):\n",
    "    \"\"\"Returns the estimated Statistical Parity of a tree for a privacy mechanism. The PAFER algorithm. \n",
    "    \n",
    "    Args:\n",
    "        pos_ruleset: A list of rules that classify favorably in the tree. This is the representation of the\n",
    "        (relevant parts of the) tree. \n",
    "        dataset: The DataFrame that the developers own that does not contain sensitive feature values.\n",
    "        sens_dataset: The Series that contains the sensitive features, which the developers do not own.\n",
    "        S: The set/list of sensitive attributes, should all be in the sens_dataset attribute.\n",
    "        mechanism: The mechanism with which to privatise the query answers. \n",
    "        epsilon: The privacy budget for the privacy mechanism. Should be larger than 0.\n",
    "        delta: The privacy margin. Ignored when mechanism is either laplacian or gaussian. Should be in (0, 1].\n",
    "        \n",
    "    Returns:\n",
    "        The statistical parity ratio for the specified pos_ruleset. \n",
    "        \"\"\"\n",
    "    \n",
    "    poscounts_per_si = np.zeros(len(S))\n",
    "    \n",
    "    # the variable name of the current dataset is inferred from the ruleset\n",
    "    datasetname = str(pos_ruleset[0].split('[')[0])[1:]\n",
    "    \n",
    "    # the base rule is a rule that includes all individuals, i.e. the condition is a tautology\n",
    "    # in this case we select all rows that have a value that is in the set of possible values of the first column\n",
    "    base_rule = f\"({datasetname}[{datasetname}.columns[0]].isin({datasetname}[{datasetname}.columns[0]].unique()))\"\n",
    "    \n",
    "    # query the size of each sensitive attribute in the dataset\n",
    "    total_per_si = [oracle(dataset, sens_dataset, base_rule, s_i, mechanism, 0.5 * epsilon, delta) for s_i in S]\n",
    "    \n",
    "    # replace each invalid value with balanced totals\n",
    "    for i, tot in enumerate(total_per_si):\n",
    "        if tot < 0 or tot > len(sens_dataset):\n",
    "            total_per_si[i] = (1 / len(S)) * len(sens_dataset)\n",
    "        \n",
    "    total_per_si = np.array(total_per_si)\n",
    "    \n",
    "    for rule in pos_ruleset:\n",
    "        # for each rule we find the distribution of sensitive attributes\n",
    "        rule_counts = np.zeros(len(S))\n",
    "        rule_total = len(sens_dataset[pd.eval(rule)])\n",
    "        \n",
    "        for i, s_i in enumerate(S):\n",
    "            # because the queries are disjoint, epsilon remains equal across queries\n",
    "            answer = round(oracle(dataset, sens_dataset, rule, s_i, mechanism, 0.5 * epsilon, delta))\n",
    "\n",
    "            # if invalid answers from query: replace with balanced node value\n",
    "            if answer < 0 or answer > len(sens_dataset):\n",
    "                answer = (1 / len(S)) * rule_total\n",
    "\n",
    "            rule_counts[i] += answer\n",
    "        \n",
    "        # the distribution for the current rule is added to the total\n",
    "        poscounts_per_si += rule_counts\n",
    "    \n",
    "    # calculate and return sp\n",
    "    accept_rates = poscounts_per_si / total_per_si\n",
    "    return np.min(accept_rates) / np.max(accept_rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43616e-3e63-44f8-b644-9ae30d2b3a14",
   "metadata": {},
   "source": [
    "## Tree construction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b51e6816-d211-4e73-951e-65a4dac88279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_tree(dataset, dataset_labels, minleaf=1, ccp_alpha=0.0):\n",
    "    \"\"\"Train a tree for balanced accuracy performance using grid search. \n",
    "    \n",
    "    dataset: The training data (without sensitive attributes!).\n",
    "    dataset_labels: The true outcomes for the prediction task.\n",
    "    minleaf: The tree construction parameter denoting the minimum number of instances in a leaf node.\n",
    "        minleaf should be in (0, 1]. \n",
    "        \n",
    "    Returns: \n",
    "        The best performing decision tree.\"\"\"\n",
    "    \n",
    "    # no random_state because we want a different tree each run\n",
    "    tree = DecisionTreeClassifier()\n",
    "\n",
    "    parameter_grid = {\"criterion\":[\"entropy\", \"gini\"],\n",
    "                      \"max_features\":[\"sqrt\", \"log2\"], \n",
    "                      \"min_samples_leaf\":[minleaf], \"ccp_alpha\": [ccp_alpha]}\n",
    "    \n",
    "    # train and return the best tree\n",
    "    tree_cv = GridSearchCV(tree, param_grid=parameter_grid, scoring='balanced_accuracy', n_jobs=2, cv=3, verbose=0)\n",
    "    tree_cv.fit(dataset, dataset_labels)\n",
    "    best_tree = tree_cv.best_estimator_\n",
    "    return best_tree\n",
    "\n",
    "best_tree = find_best_tree(taiwan_train_X, taiwan_train_y, ccp_alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bb89217-4c60-4c01-9887-79b1a45e4677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taken from: https://stackoverflow.com/a/51398390\n",
    "def is_leaf(inner_tree, index):\n",
    "    # check whether node is leaf node\n",
    "    return (inner_tree.children_left[index] == TREE_LEAF and \n",
    "            inner_tree.children_right[index] == TREE_LEAF)\n",
    "\n",
    "def prune_index(inner_tree, decisions, index=0):\n",
    "    # start pruning from the bottom - if we start from the top, we might miss\n",
    "    # nodes that become leaves during pruning\n",
    "    if not is_leaf(inner_tree, inner_tree.children_left[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_left[index])\n",
    "    if not is_leaf(inner_tree, inner_tree.children_right[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_right[index])\n",
    "\n",
    "    # prune children if both children are leaves now and make the same decision\n",
    "    if (is_leaf(inner_tree, inner_tree.children_left[index]) and\n",
    "        is_leaf(inner_tree, inner_tree.children_right[index]) and\n",
    "        (decisions[index] == decisions[inner_tree.children_left[index]]) and \n",
    "        (decisions[index] == decisions[inner_tree.children_right[index]])):\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "\n",
    "def prune_duplicate_leaves(mdl):\n",
    "    # Remove leaves if all siblings make the same decision\n",
    "    decisions = mdl.tree_.value.argmax(axis=2).flatten().tolist() # Decision for each node\n",
    "    prune_index(mdl.tree_, decisions)\n",
    "    \n",
    "# pruning happens in-place\n",
    "prune_duplicate_leaves(best_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4df0ff8-3860-48d1-a6fb-1882abf75448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positive_rules (tree, rules):\n",
    "    \"\"\"From the extracted rules, return those that have a favorable classification. \n",
    "\n",
    "    Arg:\n",
    "        tree: The tree classification object from which the rules are extracted. \n",
    "        rules: Dict of which the values are rule strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of all the rules that classify favorably\"\"\"\n",
    "\n",
    "    # only those rules are added for which the majority of individuals in the node is at index 1, i.e. max\n",
    "    # index 1 corresponds to class 1 which we ensured was the favorable outcome\n",
    "    return [rule for node_id, rule in rules.items() if np.argmax(tree.tree_.value[node_id][0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bd9053b-bf7e-437a-952c-d434b2a2c29a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"(taiwan_train_X['PAY_2']> 1.5  )\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taken from: https://stackoverflow.com/a/56427596\n",
    "def extract_pos_rules(tree, dataset, datasetname):\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    feature = tree.tree_.feature\n",
    "    threshold = tree.tree_.threshold\n",
    "\n",
    "    def find_path(node_numb, path, x):\n",
    "        path.append(node_numb)\n",
    "        if node_numb == x:\n",
    "            return True\n",
    "        left = False\n",
    "        right = False\n",
    "        if (children_left[node_numb] !=-1):\n",
    "            left = find_path(children_left[node_numb], path, x)\n",
    "        if (children_right[node_numb] !=-1):\n",
    "            right = find_path(children_right[node_numb], path, x)\n",
    "        if left or right :\n",
    "            return True\n",
    "        path.remove(node_numb)\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_rule(datasetname, path, column_names):\n",
    "        mask = '('\n",
    "        for index, node in enumerate(path):\n",
    "            # check if we are not in the leaf\n",
    "            if index!=len(path)-1:\n",
    "                # under or over the threshold?\n",
    "                if (children_left[node] == path[index+1]):\n",
    "                    mask += f\"{datasetname}['{column_names[feature[node]]}']<= {threshold[node]}\\t \"\n",
    "                else:\n",
    "                    mask += f\"{datasetname}['{column_names[feature[node]]}']> {threshold[node]} \\t \"\n",
    "\n",
    "        # insert the & at the right places\n",
    "        mask = mask.replace(\"\\t\", \"&\", mask.count(\"\\t\") - 1)\n",
    "        mask = mask.replace(\"\\t\", \"\")\n",
    "        mask += \")\"\n",
    "        return mask\n",
    "    \n",
    "    # Leaves\n",
    "    leave_id = tree.apply(dataset)\n",
    "\n",
    "    paths = {}\n",
    "    for leaf in np.unique(leave_id):\n",
    "        path_leaf = []\n",
    "        find_path(0, path_leaf, leaf)\n",
    "        paths[leaf] = np.unique(np.sort(path_leaf))\n",
    "\n",
    "    rules = {}\n",
    "    for key in paths:\n",
    "        rules[key] = get_rule(datasetname, paths[key], [name for name in dataset.columns])\n",
    "        \n",
    "    return positive_rules(tree, rules)\n",
    "        \n",
    "extract_pos_rules(best_tree, taiwan_train_X, \"taiwan_train_X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d20e5d-7bfb-45c6-864a-9423e7e7b52b",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "930b536a-0832-4a27-8894-70f92a5b80f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootstrap(dataset, dataset_labels, sens_dataset):\n",
    "    \"\"\"A bootstrapping function that helps to diversify the tree generating process.\"\"\"\n",
    "    indices = np.random.choice(dataset.index, size=len(dataset.index))\n",
    "    \n",
    "    return dataset.iloc[indices], dataset_labels.iloc[indices], sens_dataset.iloc[indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df994e20-79e9-47e1-b4f4-c091a2b9f299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experiment(trainset, sens_trainset, trainsetname, trainset_labels, testset, sens_testset, testsetname, \n",
    "               testset_labels, epsilons=[0.05, 0.1, 0.15, 0.2, 0.25], minleaf=1, ccp_alpha=0.0, runs=5, combined=False):\n",
    "    \"\"\"Performs an experiment as described in the paper.\n",
    "    \n",
    "    trainset: The DataFrame containing the training instances.\n",
    "    sens_trainset: The Series containing the sensitive attribute values for the trainset.\n",
    "    trainsetname: The variable name of the trainset. Required because of rule evaluation.\n",
    "    trainset_labels: The true outcomes for the prediction task for the trainset.\n",
    "    testset: The DataFrame containing the test instances.\n",
    "    sens_testset: The Series containing the sensitive attribute values for the testset.\n",
    "    testsetname: The variable name of the testset. Required because of rule evaluation.\n",
    "    testset_labels: The true outcomes for the prediction task for the testset.\n",
    "    epsilons: The different privacy budgets to try. \n",
    "        epsilons must be a list.\n",
    "    minleaf: The tree construction parameter denoting the minimum number of instances in a leaf node.\n",
    "        minleaf should be in (0, 1].\n",
    "    runs: The number of runs to average over. Advised to be quite high (e.g. 50) to compensate for noise.\n",
    "    combined: Whether to combine all positive rules into one query. \n",
    "    \n",
    "    Returns:\n",
    "        The true SP of the tree and the estimated SP.\"\"\"\n",
    "    \n",
    "    \n",
    "    tree_sps = np.zeros((runs, len(epsilons)))\n",
    "    tree_depths = np.zeros((runs, len(epsilons)))\n",
    "    estimated_sps = np.zeros((runs, len(epsilons)))\n",
    "    for i in range(runs):\n",
    "        ruleset = []\n",
    "        \n",
    "        # keep boostrapping until we find a ruleset that has at least one positive rule\n",
    "        while ruleset == [] or ruleset == ['()']:\n",
    "            # sample with replacement\n",
    "            dataset, dataset_labels, sens_dataset = bootstrap(trainset, trainset_labels, sens_trainset)\n",
    "            \n",
    "            # build tree \n",
    "            best_tree = find_best_tree(trainset, trainset_labels, minleaf, ccp_alpha)\n",
    "        \n",
    "            # extract positive rules\n",
    "            prune_duplicate_leaves(best_tree)\n",
    "            ruleset = extract_pos_rules(best_tree, trainset, testsetname)\n",
    "        \n",
    "        if combined:\n",
    "            ruleset = [\" | \".join(rule for rule in ruleset)]\n",
    "\n",
    "        # calculate true SP\n",
    "        tree_sps[i] = statistical_parity(best_tree.predict(testset), sens_testset)\n",
    "        tree_depths[i] = best_tree.tree_.max_depth\n",
    "        \n",
    "        # apply PAFER\n",
    "        for j, epsilon in enumerate(epsilons):\n",
    "            estimated_sps[i, j] = estimate_sp(ruleset, testset, sens_testset, sorted(sens_testset.unique()), mechanism='laplacian', epsilon=epsilon)\n",
    "        \n",
    "    return tree_sps, tree_depths, estimated_sps\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db26183-dd9e-4c03-983b-3cf4b754e3cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MINLEAF EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16fc19a8-6256-4e13-b3ea-c5df3c7052b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [56:04<00:00, 336.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# script to run the experiments\n",
    "# TODO: current datastructures are not optimal or intuitive so could be helpful to streamline (for plotting)\n",
    "minleafs = np.linspace(0.2, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for minleaf in tqdm(minleafs):\n",
    "    t_sps, t_depths, e_sps = experiment(taiwan_train_X, taiwan_train_sex, \"taiwan_train_X\", taiwan_train_y, \n",
    "                                         taiwan_test_X, taiwan_test_sex, \"taiwan_test_X\", taiwan_test_y, minleaf=minleaf, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f73964-d83a-4369-a412-fbc4e50269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"taiwan-sex-minleaf-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8aef5-d12e-4032-a6d7-00313f6f174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to run the experiments\n",
    "ccp_alphas = np.linspace(0.05, 0.001, 80)\n",
    "runs = 50\n",
    "\n",
    "# storage for results\n",
    "tree_sps = []\n",
    "tree_depths = []\n",
    "estimated_sps = []\n",
    "for ccp_alpha in tqdm(ccp_alphas):\n",
    "    t_sps, t_depths, e_sps = experiment(taiwan_train_X, taiwan_train_sex, \"taiwan_train_X\", taiwan_train_y, \n",
    "                                         taiwan_test_X, taiwan_test_sex, \"taiwan_test_X\", taiwan_test_y, ccp_alpha=ccp_alpha, runs=runs)\n",
    "    \n",
    "    tree_sps.append(t_sps)\n",
    "    tree_depths.append(t_depths)\n",
    "    estimated_sps.append(e_sps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd98a35-9aa3-40f3-8d0c-062a9be9b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sps = np.array(tree_sps)\n",
    "tree_depths = np.array(tree_depths)\n",
    "estimated_sps = np.array(estimated_sps)\n",
    "\n",
    "for arr, name in zip([tree_sps, tree_depths, estimated_sps], [\"tree_sps\", \"tree_depths\", \"estimated_sps\"]):\n",
    "    with open(f\"taiwan-sex-ccp-{name}\", \"wb\") as f:\n",
    "        np.save(f, arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
